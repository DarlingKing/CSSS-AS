{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2, exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing of packages\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import math\n",
    "from wordcloud import WordCloud\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "stopwords=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "df = pd.read_csv('/Users/espensivertsen/Downloads/wallstreet_subs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation and implementation of the 'textsubmission' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation and implementation of the 'textsubmission' column\n",
    "x=df['title']\n",
    "y=df['selftext']\n",
    "x_1=[]\n",
    "y_1=[]\n",
    "for i in range(len(df)):\n",
    "    x_1.append(x[i])\n",
    "    y_1.append(y[i])\n",
    "list1=[]\n",
    "for i in range(len(df)):\n",
    "    list1.append(x_1[i]+' '+y_1[i])\n",
    "    \n",
    "df['textsubmission']=list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding every word starting with '$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=str(list(df['textsubmission']))\n",
    "reg_ex=('\\$\\w+')\n",
    "\n",
    "c=re.findall(reg_ex,b) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the '$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [e[1:] for e in c] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Removing every element containing a number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [x for x in c if not any(x1.isdigit() for x1 in x)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making everything uppercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_list=[x.upper() for x in test] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the top 15 tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPY',\n",
       " 'TSLA',\n",
       " 'SPCE',\n",
       " 'PLTR',\n",
       " 'MSFT',\n",
       " 'ROPE',\n",
       " 'AAPL',\n",
       " 'AMZN',\n",
       " 'NIO',\n",
       " 'ZM',\n",
       " 'AMD',\n",
       " 'BABA',\n",
       " 'GME',\n",
       " 'DIS',\n",
       " 'BA']"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter = {}\n",
    "for word in main_list:\n",
    "    if word in word_counter:\n",
    "        word_counter[word] += 1\n",
    "    else:\n",
    "         word_counter[word] = 1\n",
    "\n",
    "popular_words = sorted(word_counter, key = word_counter.get, reverse = True)\n",
    "top_15 = popular_words[:15]\n",
    "top_15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Tokenizing and cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the \"selftext\" column a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_selftext=[]\n",
    "for i in range(len(df['selftext'])):\n",
    "    liste_selftext.append(df['selftext'][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing URLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_selftext_1=[]\n",
    "for i in range(len(liste_selftext)):\n",
    "    liste_selftext_1.append(re.sub(r'http\\S+','',str(liste_selftext[i])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(list):\n",
    "    pattern = '[0-9]'\n",
    "    list = [re.sub(pattern, '', i) for i in list]\n",
    "    return list\n",
    "\n",
    "liste_selftext_2 = remove(liste_selftext_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making everything lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_selftext_3=[x.lower() for x in liste_selftext_2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word not in stopwords:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "liste_selftext_4=remove_stopwords(liste_selftext_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the list and removing punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "liste_token=[]\n",
    "for i in range(len(df['selftext'])):\n",
    "    liste_token.append(tokenizer.tokenize(str(liste_selftext_4[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove stopwords that were \"hidden\" in punctuation  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_token2=[]\n",
    "for i in range(len(liste_token)):\n",
    "    test=[]\n",
    "    for j in liste_token[i]:\n",
    "        if j not in stopwords:\n",
    "            test.append(j)\n",
    "    liste_token2.append(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing duplicates within the same sublist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokens = []\n",
    "for i in range(len(liste_token2)):\n",
    "    a=[]\n",
    "    for elem in liste_token2[i]:\n",
    "        if elem not in a:\n",
    "            a.append(elem)\n",
    "    list_tokens.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the tokens column to the df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens']=list_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Adding the stocks to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the tags lower case so that they match the tokens (which we made all lower case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_lower=[x.lower() for x in top_15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list of the stocks within the tokens list (when there is no tag the list is empty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stocks=[]\n",
    "for i in range(len(list_tokens)):\n",
    "    append_list=[]\n",
    "    for j in range(len(top_15_lower)):\n",
    "            if top_15_lower[j] in list_tokens[i]:\n",
    "                append_list.append(top_15_lower[j])\n",
    "    list_stocks.append(append_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replacing all the empty items with 'Other'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_stocks)):\n",
    "    if list_stocks[i]==[]:\n",
    "        list_stocks[i]= ['Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the list of stocks to the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stocks']=list_stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Creation of large documents for each stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list for every words for each tag + 'Other'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ba=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'ba' in list_stocks[i]:\n",
    "        words_ba.append(list_tokens[i])\n",
    "\n",
    "words_spy=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'spy' in list_stocks[i]:\n",
    "        words_spy.append(list_tokens[i])\n",
    "\n",
    "words_tsla=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'tsla' in list_stocks[i]:\n",
    "          words_tsla.append(list_tokens[i])\n",
    "            \n",
    "words_spce=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'spce' in list_stocks[i]:\n",
    "        words_spce.append(list_tokens[i])\n",
    "        \n",
    "words_pltr=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'pltr' in list_stocks[i]:\n",
    "        words_pltr.append(list_tokens[i])\n",
    "        \n",
    "words_msft=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'msft' in list_stocks[i]:\n",
    "        words_msft.append(list_tokens[i])\n",
    "        \n",
    "words_rope=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'rope' in list_stocks[i]:\n",
    "        words_rope.append(list_tokens[i])\n",
    "        \n",
    "words_aapl=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'aapl' in list_stocks[i]:\n",
    "        words_aapl.append(list_tokens[i])\n",
    "        \n",
    "words_amzn=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'amzn' in list_stocks[i]:\n",
    "        words_amzn.append(list_tokens[i])\n",
    "        \n",
    "words_nio=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'nio' in list_stocks[i]:\n",
    "        words_nio.append(list_tokens[i])\n",
    "        \n",
    "words_zm=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'zm' in list_stocks[i]:\n",
    "        words_zm.append(list_tokens[i])\n",
    "        \n",
    "words_amd=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'amd' in list_stocks[i]:\n",
    "        words_amd.append(list_tokens[i])\n",
    "        \n",
    "words_baba=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'baba' in list_stocks[i]:\n",
    "        words_baba.append(list_tokens[i])\n",
    "        \n",
    "words_gme=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'gme' in list_stocks[i]:\n",
    "        words_gme.append(list_tokens[i])\n",
    "        \n",
    "words_dis=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'dis' in list_stocks[i]:\n",
    "        words_dis.append(list_tokens[i])\n",
    "        \n",
    "words_Other=[]\n",
    "for i in range(len(list_stocks)):\n",
    "    if 'Other' in list_stocks[i]:\n",
    "        words_Other.append(list_tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing non english words from the tokens and flattening the list for the 5 chosen tags (couldnt remove non-english words when cleaning of the tokens because tags such as 'tsla', 'gme' would get removed then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_gme1=[]\n",
    "for i in range(len(words_gme)):\n",
    "    test2=[]\n",
    "    for j in words_gme[i]:\n",
    "        if j in english_words:\n",
    "            test2.append(j)\n",
    "    words_gme1.append(test2)\n",
    "words_gme_flat = [x for l in words_gme1 for x in l]\n",
    "\n",
    "words_spy1=[]\n",
    "for i in range(len(words_spy)):\n",
    "    test3=[]\n",
    "    for j in words_spy[i]:\n",
    "        if j in english_words:\n",
    "            test3.append(j)\n",
    "    words_spy1.append(test3)\n",
    "words_spy_flat = [x for l in words_spy1 for x in l]\n",
    "\n",
    "words_tsla1=[]\n",
    "for i in range(len(words_tsla)):\n",
    "    test4=[]\n",
    "    for j in words_tsla[i]:\n",
    "        if j in english_words:\n",
    "            test4.append(j)\n",
    "    words_tsla1.append(test4)\n",
    "words_tsla_flat = [x for l in words_tsla1 for x in l]\n",
    "\n",
    "words_spce1=[]\n",
    "for i in range(len(words_spce)):\n",
    "    test5=[]\n",
    "    for j in words_spce[i]:\n",
    "        if j in english_words:\n",
    "            test5.append(j)\n",
    "    words_spce1.append(test5)\n",
    "words_spce_flat = [x for l in words_spce1 for x in l]\n",
    "\n",
    "words_amzn1=[]\n",
    "for i in range(len(words_amzn)):\n",
    "    test6=[]\n",
    "    for j in words_amzn[i]:\n",
    "        if j in english_words:\n",
    "            test6.append(j)\n",
    "    words_amzn1.append(test6)\n",
    "words_amzn_flat = [x for l in words_amzn1 for x in l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: TF and IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the stocks: 'gme', 'spy', 'tsla', 'spce' and 'amzn' and finding the 5 most common terms from the token list and how many times they occur for each chosen stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gme\n",
    "words_to_count_gme = (word for word in words_gme_flat)\n",
    "top_gme=Counter(words_to_count_gme).most_common(len(words_gme_flat))\n",
    "\n",
    "#spy\n",
    "words_to_count_spy = (word for word in words_spy_flat)\n",
    "top_spy=Counter(words_to_count_spy).most_common(len(words_spy_flat))\n",
    "\n",
    "#tsla\n",
    "words_to_count_tsla = (word for word in words_tsla_flat)\n",
    "top_tsla=Counter(words_to_count_tsla).most_common(len(words_tsla_flat))\n",
    "\n",
    "#spce\n",
    "words_to_count_spce = (word for word in words_spce_flat)\n",
    "top_spce=Counter(words_to_count_spce).most_common(len(words_spce_flat))\n",
    "\n",
    "#amzn\n",
    "words_to_count_amzn = (word for word in words_amzn_flat)\n",
    "top_amzn=Counter(words_to_count_amzn).most_common(len(words_amzn_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('buy', 409), ('short', 393), ('going', 388), ('like', 387), ('get', 378)]\n",
      "[('spy', 7038), ('market', 2447), ('like', 2242), ('p', 2129), ('going', 1939)]\n",
      "[('like', 1249), ('buy', 1094), ('get', 1032), ('market', 1020), ('stock', 1017)]\n",
      "[('like', 391), ('go', 364), ('stock', 335), ('buy', 333), ('get', 331)]\n",
      "[('like', 343), ('market', 299), ('going', 265), ('time', 262), ('one', 260)]\n"
     ]
    }
   ],
   "source": [
    "print(top_gme[0:5])\n",
    "print(top_spy[0:5])\n",
    "print(top_tsla[0:5])\n",
    "print(top_spce[0:5])\n",
    "print(top_amzn[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe similarities and differences between the stocks:\n",
    "We can see from the results that a lot of the same words go again and again such ass 'buy', 'going', 'like', 'stock', 'market' which makes since this subreddit is about the stock market. How the 'p' appeared am I not sure of, the 'p' is not in the stopwords list, maybe its an abbreviation for something like prize or something? Other than that all the result are quite alike, and expectable.\n",
    "\n",
    "Why aren't the TFs not necessarily a good description of the stocks?\n",
    "TF is how many times a word appears in a text. When discussing on reddit a lot of words not necessarily used to desribe the stock may be used. Even though we've done some cleaning steps there will be words left. It's alot of talk about 'buying', 'going', the 'market' etc which will occur just because the stocks are talked about which isnt really describing the stock.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the TF (number the given word occurs / total words in the document) for the top 5 tokens for the 5 chosen stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gme\n",
    "tf_gme=[]\n",
    "for i in range(len(top_gme)):\n",
    "    tf_gme.append((top_gme[i][0], top_gme[i][1]/len(words_gme_flat)))\n",
    "\n",
    "#spy\n",
    "tf_spy=[]\n",
    "for i in range(len(top_spy)):\n",
    "    tf_spy.append((top_spy[i][0], top_spy[i][1]/len(words_spy_flat)))\n",
    "    \n",
    "#tsla\n",
    "tf_tsla=[]\n",
    "for i in range(len(top_tsla)):\n",
    "    tf_tsla.append((top_tsla[i][0], top_tsla[i][1]/len(words_tsla_flat)))\n",
    "    \n",
    "#spce\n",
    "tf_spce=[]\n",
    "for i in range(len(top_spce)):\n",
    "    tf_spce.append((top_spce[i][0], top_spce[i][1]/len(words_spce_flat)))\n",
    "    \n",
    "#amzn\n",
    "tf_amzn=[]\n",
    "for i in range(len(top_amzn)):\n",
    "    tf_amzn.append((top_amzn[i][0], top_amzn[i][1]/len(words_amzn_flat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('buy', 0.005571144468357534), ('short', 0.0053532023864657965), ('going', 0.005285095485874629), ('like', 0.005271474105756395), ('get', 0.005148881684692293)]\n",
      "[('spy', 0.018888635418204264), ('market', 0.006567276338213389), ('like', 0.006017095852175897), ('p', 0.005713825633043035), ('going', 0.005203902255739992)]\n",
      "[('like', 0.005998031070665354), ('buy', 0.005253679736835787), ('get', 0.004955939203303959), ('market', 0.004898312003265542), ('stock', 0.004883905203255936)]\n",
      "[('like', 0.0067212156633547635), ('go', 0.00625709079657579), ('stock', 0.005758586310035411), ('buy', 0.005724206690274005), ('get', 0.0056898270705126)]\n",
      "[('like', 0.004922573515693394), ('market', 0.0042911063591612965), ('going', 0.0038031544654774036), ('time', 0.0037600998866229424), ('one', 0.0037313968340533014)]\n"
     ]
    }
   ],
   "source": [
    "print(tf_gme[0:5])\n",
    "print(tf_spy[0:5])\n",
    "print(tf_tsla[0:5])\n",
    "print(tf_spce[0:5])\n",
    "print(tf_amzn[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation by using the formula: log(Number of documents/number of documents the word appear in). where the number of documents will be the length of the list_tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to find number of documents the words appear in. (Since IDF for one word is the same, only needs to calculate one time each word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buy\n",
    "docs_with_buy=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'buy' in list_tokens[i]:\n",
    "        docs_with_buy+=1\n",
    "#short\n",
    "docs_with_short=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'short' in list_tokens[i]:\n",
    "        docs_with_short+=1\n",
    "#going        \n",
    "docs_with_going=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'going' in list_tokens[i]:\n",
    "        docs_with_going+=1\n",
    "#like\n",
    "docs_with_like=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'like' in list_tokens[i]:\n",
    "        docs_with_like+=1        \n",
    "#get\n",
    "docs_with_get=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'get' in list_tokens[i]:\n",
    "        docs_with_get+=1\n",
    "#spy\n",
    "docs_with_spy=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'spy' in list_tokens[i]:\n",
    "        docs_with_spy+=1        \n",
    "#market\n",
    "docs_with_market=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'market' in list_tokens[i]:\n",
    "        docs_with_market+=1        \n",
    "#p\n",
    "docs_with_p=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'p' in list_tokens[i]:\n",
    "        docs_with_p+=1\n",
    "#stock\n",
    "docs_with_stock=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'stock' in list_tokens[i]:\n",
    "        docs_with_stock+=1\n",
    "#go\n",
    "docs_with_go=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'go' in list_tokens[i]:\n",
    "        docs_with_go+=1       \n",
    "#time\n",
    "docs_with_time=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'time' in list_tokens[i]:\n",
    "        docs_with_time+=1\n",
    "#one\n",
    "docs_with_one=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'one' in list_tokens[i]:\n",
    "        docs_with_one+=1\n",
    "\n",
    "#know\n",
    "docs_with_know=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'know' in list_tokens[i]:\n",
    "        docs_with_know+=1\n",
    "#money\n",
    "docs_with_money=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'money' in list_tokens[i]:\n",
    "        docs_with_money+=1\n",
    "#would\n",
    "docs_with_would=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'would' in list_tokens[i]:\n",
    "        docs_with_would+=1\n",
    "#also\n",
    "docs_with_also=0\n",
    "for i in range(len(list_tokens)):\n",
    "    if 'also' in list_tokens[i]:\n",
    "        docs_with_also+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a list of all the numbers from above to have easier code to iterate through\n",
    "list_with_numbers=[('buy', docs_with_buy), ('short', docs_with_short), ('going', docs_with_going), ('like',docs_with_one),('get',docs_with_get),('spy',docs_with_spy),('market',docs_with_market),('p',docs_with_p),('stock',docs_with_stock),('go',docs_with_go),('time',docs_with_time),('one',docs_with_one),('know',docs_with_know),('money',docs_with_money),('would',docs_with_would),('also', docs_with_also) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buy', 1.7092449465395803),\n",
       " ('short', 2.516102451794195),\n",
       " ('going', 1.6616262711589307),\n",
       " ('like', 1.8765250164535898),\n",
       " ('get', 1.6292332442144688),\n",
       " ('spy', 2.458342081577906),\n",
       " ('market', 1.632774170417683),\n",
       " ('p', 2.5026453109141555),\n",
       " ('stock', 1.7644847241540806),\n",
       " ('go', 1.738141600724539),\n",
       " ('time', 1.7769143434253172),\n",
       " ('one', 1.8765250164535898),\n",
       " ('know', 1.7780650915384952),\n",
       " ('money', 1.7356554115986131),\n",
       " ('would', 1.9368320519235682),\n",
       " ('also', 2.129326636430573)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDF_values=[]\n",
    "for i in range(len(list_with_numbers)):\n",
    "    IDF_values.append((list_with_numbers[i][0],math.log(len(list_tokens)/list_with_numbers[i][1])))\n",
    "\n",
    "IDF_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 tf for the 5 stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('buy', 0.005571144468357534), ('short', 0.0053532023864657965), ('going', 0.005285095485874629), ('like', 0.005271474105756395), ('get', 0.005148881684692293), ('know', 0.00479472580161822), ('stock', 0.00464489062031765), ('time', 0.004440569918544147), ('go', 0.004345220257716512), ('one', 0.004249870596888877)]\n",
      "[('spy', 0.018888635418204264), ('market', 0.006567276338213389), ('like', 0.006017095852175897), ('p', 0.005713825633043035), ('going', 0.005203902255739992), ('get', 0.005123388038271092), ('go', 0.005104601387528347), ('money', 0.004887213000362314), ('time', 0.00481206639739134), ('buy', 0.004656405576951463)]\n",
      "[('like', 0.005998031070665354), ('buy', 0.005253679736835787), ('get', 0.004955939203303959), ('market', 0.004898312003265542), ('stock', 0.004883905203255936), ('going', 0.004715825869810551), ('go', 0.0045621533363747685), ('money', 0.00453333973635556), ('one', 0.004514130669676087), ('time', 0.004485317069656878)]\n",
      "[('like', 0.0067212156633547635), ('go', 0.00625709079657579), ('stock', 0.005758586310035411), ('buy', 0.005724206690274005), ('get', 0.0056898270705126), ('going', 0.0056210678309897896), ('money', 0.005242892013614329), ('market', 0.004761577336954653), ('time', 0.0047443875270739505), ('one', 0.004349021899817788)]\n",
      "[('like', 0.004922573515693394), ('market', 0.0042911063591612965), ('going', 0.0038031544654774036), ('time', 0.0037600998866229424), ('one', 0.0037313968340533014), ('stock', 0.0036165846237747384), ('would', 0.003530475466065816), ('get', 0.0035017724134961755), ('also', 0.003487420887211355), ('buy', 0.0034300147820720732)]\n"
     ]
    }
   ],
   "source": [
    "print(tf_gme[0:10])\n",
    "print(tf_spy[0:10])\n",
    "print(tf_tsla[0:10])\n",
    "print(tf_spce[0:10])\n",
    "print(tf_amzn[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function makes it able to sort by the 2nd element, the tf and not \n",
    "#alphabetically by words\n",
    "def takeSecond(elem):\n",
    "    return elem[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 TF-IDF for $GME. Calculated by TF*IDF for the given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('short', 0.013469205649537127),\n",
       " ('like', 0.009892053033039193),\n",
       " ('buy', 0.009522450528982053),\n",
       " ('going', 0.008781853504912757),\n",
       " ('know', 0.008525334571356286),\n",
       " ('get', 0.008388729211227686),\n",
       " ('stock', 0.008195838544917065),\n",
       " ('one', 0.007974988491752528),\n",
       " ('time', 0.007890512381244087),\n",
       " ('go', 0.007552608094248071)]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF_gme=[]\n",
    "for i in range(len(tf_gme[0:10])):\n",
    "    for j in range(len(IDF_values)):\n",
    "        if tf_gme[i][0]==IDF_values[j][0]:\n",
    "            TF_IDF_gme.append((tf_gme[i][0], tf_gme[i][1]*IDF_values[j][1]))\n",
    "\n",
    "TF_IDF_gme.sort(key=takeSecond, reverse=True)\n",
    "TF_IDF_gme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 TF-IDF for $SPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spy', 0.04643472731215443),\n",
       " ('p', 0.01429967892791626),\n",
       " ('like', 0.011291230893007202),\n",
       " ('market', 0.010722879175030045),\n",
       " ('go', 0.008872520026779225),\n",
       " ('going', 0.008646940700680792),\n",
       " ('time', 0.008550629803039663),\n",
       " ('money', 0.008482517691713945),\n",
       " ('get', 0.008347194114962014),\n",
       " ('buy', 0.007958937701443007)]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF_spy=[]\n",
    "for i in range(len(tf_spy[0:10])):\n",
    "    for j in range(len(IDF_values)):\n",
    "        if tf_spy[i][0]==IDF_values[j][0]:\n",
    "            TF_IDF_spy.append((tf_spy[i][0], tf_spy[i][1]*IDF_values[j][1]))\n",
    "\n",
    "TF_IDF_spy.sort(key=takeSecond, reverse=True)\n",
    "TF_IDF_spy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 TF-IDF for $TSLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 0.011255455353569446),\n",
       " ('buy', 0.00897982554092396),\n",
       " ('stock', 0.00861757612536173),\n",
       " ('one', 0.008470879129187574),\n",
       " ('get', 0.008074380906328579),\n",
       " ('market', 0.007997837317578874),\n",
       " ('time', 0.00797002423588372),\n",
       " ('go', 0.007929668502837236),\n",
       " ('money', 0.007868315646020557),\n",
       " ('going', 0.007835940155488127)]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF_tsla=[]\n",
    "for i in range(len(tf_tsla[0:10])):\n",
    "    for j in range(len(IDF_values)):\n",
    "        if tf_tsla[i][0]==IDF_values[j][0]:\n",
    "            TF_IDF_tsla.append((tf_tsla[i][0], tf_tsla[i][1]*IDF_values[j][1]))\n",
    "\n",
    "TF_IDF_tsla.sort(key=takeSecond, reverse=True)\n",
    "TF_IDF_tsla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 TF-IDF for $SPCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 0.012612529333264923),\n",
       " ('go', 0.010875709813039024),\n",
       " ('stock', 0.010160937576780297),\n",
       " ('buy', 0.0097840713582989),\n",
       " ('going', 0.009340113979938983),\n",
       " ('get', 0.00927005541711055),\n",
       " ('money', 0.00909985389585686),\n",
       " ('time', 0.008430370247625872),\n",
       " ('one', 0.008161048392112596),\n",
       " ('market', 0.007774580486225774)]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF_spce=[]\n",
    "for i in range(len(tf_spce[0:10])):\n",
    "    for j in range(len(IDF_values)):\n",
    "        if tf_spce[i][0]==IDF_values[j][0]:\n",
    "            TF_IDF_spce.append((tf_spce[i][0], tf_spce[i][1]*IDF_values[j][1]))\n",
    "\n",
    "TF_IDF_spce.sort(key=takeSecond, reverse=True)\n",
    "TF_IDF_spce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 TF-IDF for $AMZN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 0.009237332347530552),\n",
       " ('also', 0.007425858187583479),\n",
       " ('market', 0.0070064076257536296),\n",
       " ('one', 0.007002059505416745),\n",
       " ('would', 0.006837938041206071),\n",
       " ('time', 0.006681375421252215),\n",
       " ('stock', 0.006381408322261059),\n",
       " ('going', 0.006319421373112654),\n",
       " ('buy', 0.005862735432812751),\n",
       " ('get', 0.005705204029741104)]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF_amzn=[]\n",
    "for i in range(len(tf_amzn[0:10])):\n",
    "    for j in range(len(IDF_values)):\n",
    "        if tf_amzn[i][0]==IDF_values[j][0]:\n",
    "            TF_IDF_amzn.append((tf_amzn[i][0], tf_amzn[i][1]*IDF_values[j][1]))\n",
    "\n",
    "TF_IDF_amzn.sort(key=takeSecond, reverse=True)\n",
    "TF_IDF_amzn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I only have the IDF for the topwords the words doesnt change, but the order of them change. This makes sense as now they are dependant on other documents aswell.\n",
    "\n",
    "(was told to only find the IDF for the top words of each stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the wordcloud. The wordclouds can be found in a seperate folder in the assignment.\n",
    "I did one wordcloud based on TF-IDF and one based of TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc= WordCloud(\n",
    "    background_color='white',\n",
    "    height=600,\n",
    "    width=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GME:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f90b72b8340>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.generate(str(TF_IDF_gme))\n",
    "wc.to_file('GME.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f90b72b8340>"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.generate(str(top_gme))\n",
    "wc.to_file('GME_tf.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f90b72b8340>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.generate(str(TF_IDF_spy))\n",
    "wc.to_file('SPY.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f90b72b8340>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.generate(str(top_spy))\n",
    "wc.to_file('SPY_tf.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSLA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f90b72b8340>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.generate(str(TF_IDF_tsla))\n",
    "wc.to_file('TSLA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f90b72b8340>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.generate(str(top_tsla))\n",
    "wc.to_file('TSLA_tf.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f90b72b8340>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.generate(str(TF_IDF_spce))\n",
    "wc.to_file('spce.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f90b72b8340>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.generate(str(top_spce))\n",
    "wc.to_file('SPCE_tf.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AMZN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f90b72b8340>"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.generate(str(TF_IDF_amzn))\n",
    "wc.to_file('AMZN.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f90b72b8340>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.generate(str(top_amzn))\n",
    "wc.to_file('AMZN_tf.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly I've just added the dataframe to show that the textsubmission, tokens and stocks columns has been added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>textsubmission</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stocks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1586173811</td>\n",
       "      <td>What is the Fed actually buying?</td>\n",
       "      <td>Okay, I may actually just be retarded. On my d...</td>\n",
       "      <td>1</td>\n",
       "      <td>What is the Fed actually buying? Okay, I may a...</td>\n",
       "      <td>[okay, may, actually, retarded, defense, every...</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1586173320</td>\n",
       "      <td>I didnâ€™t learn about puts because I was lazy</td>\n",
       "      <td>Beginning of the this virus shit, everyone was...</td>\n",
       "      <td>1</td>\n",
       "      <td>I didnâ€™t learn about puts because I was lazy B...</td>\n",
       "      <td>[beginning, virus, shit, everyone, talking, pu...</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1586173268</td>\n",
       "      <td>HOT TAKE</td>\n",
       "      <td>Literally everyone has free time on their hand...</td>\n",
       "      <td>1</td>\n",
       "      <td>HOT TAKE Literally everyone has free time on t...</td>\n",
       "      <td>[literally, everyone, free, time, hands, mean,...</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1586172639</td>\n",
       "      <td>Fuck you Gordon</td>\n",
       "      <td>Gordon I believed in you, I can't even begin t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Fuck you Gordon Gordon I believed in you, I ca...</td>\n",
       "      <td>[gordon, believed, even, begin, describe, disa...</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1586171822</td>\n",
       "      <td>Canâ€™t find a picture</td>\n",
       "      <td>Someone uploaded a ohoto of the stock market h...</td>\n",
       "      <td>1</td>\n",
       "      <td>Canâ€™t find a picture Someone uploaded a ohoto ...</td>\n",
       "      <td>[someone, uploaded, ohoto, stock, market, hist...</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82237</th>\n",
       "      <td>1602007302</td>\n",
       "      <td>Hurricane Delta (BECN) ðŸ”¥</td>\n",
       "      <td>\\nHurricane Delta is looking like it is going ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hurricane Delta (BECN) ðŸ”¥ \\nHurricane Delta is ...</td>\n",
       "      <td>[hurricane, delta, looking, like, going, hit, ...</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82238</th>\n",
       "      <td>1602006818</td>\n",
       "      <td>Made 40k on Nike. Next play? CROCS motherfucker</td>\n",
       "      <td>#  1. Introduction\\n\\n[Proof that I'm lucky](h...</td>\n",
       "      <td>1</td>\n",
       "      <td>Made 40k on Nike. Next play? CROCS motherfucke...</td>\n",
       "      <td>[introduction, proof, lucky, way, going, struc...</td>\n",
       "      <td>[spy, tsla]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82239</th>\n",
       "      <td>1602006029</td>\n",
       "      <td>Please screenshot the whole timeline, not just...</td>\n",
       "      <td>I could nut over your retarded failures just f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Please screenshot the whole timeline, not just...</td>\n",
       "      <td>[could, nut, retarded, failures, fine, really,...</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82240</th>\n",
       "      <td>1602005968</td>\n",
       "      <td>What is your price target for Tesla in 40 years?</td>\n",
       "      <td>I am 26 and currently max out my roth each yea...</td>\n",
       "      <td>1</td>\n",
       "      <td>What is your price target for Tesla in 40 year...</td>\n",
       "      <td>[currently, max, roth, year, robinhood, specul...</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82241</th>\n",
       "      <td>1601822856</td>\n",
       "      <td>White House infected folks will infect Senators</td>\n",
       "      <td>They all gonna get the rona, \\n\\nThis means no...</td>\n",
       "      <td>1</td>\n",
       "      <td>White House infected folks will infect Senator...</td>\n",
       "      <td>[gonna, get, rona, means, checkies, stimulus, ...</td>\n",
       "      <td>[Other]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82242 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       created_utc                                              title  \\\n",
       "0       1586173811                   What is the Fed actually buying?   \n",
       "1       1586173320       I didnâ€™t learn about puts because I was lazy   \n",
       "2       1586173268                                           HOT TAKE   \n",
       "3       1586172639                                    Fuck you Gordon   \n",
       "4       1586171822                               Canâ€™t find a picture   \n",
       "...            ...                                                ...   \n",
       "82237   1602007302                           Hurricane Delta (BECN) ðŸ”¥   \n",
       "82238   1602006818    Made 40k on Nike. Next play? CROCS motherfucker   \n",
       "82239   1602006029  Please screenshot the whole timeline, not just...   \n",
       "82240   1602005968   What is your price target for Tesla in 40 years?   \n",
       "82241   1601822856    White House infected folks will infect Senators   \n",
       "\n",
       "                                                selftext  score  \\\n",
       "0      Okay, I may actually just be retarded. On my d...      1   \n",
       "1      Beginning of the this virus shit, everyone was...      1   \n",
       "2      Literally everyone has free time on their hand...      1   \n",
       "3      Gordon I believed in you, I can't even begin t...      1   \n",
       "4      Someone uploaded a ohoto of the stock market h...      1   \n",
       "...                                                  ...    ...   \n",
       "82237  \\nHurricane Delta is looking like it is going ...      1   \n",
       "82238  #  1. Introduction\\n\\n[Proof that I'm lucky](h...      1   \n",
       "82239  I could nut over your retarded failures just f...      1   \n",
       "82240  I am 26 and currently max out my roth each yea...      1   \n",
       "82241  They all gonna get the rona, \\n\\nThis means no...      1   \n",
       "\n",
       "                                          textsubmission  \\\n",
       "0      What is the Fed actually buying? Okay, I may a...   \n",
       "1      I didnâ€™t learn about puts because I was lazy B...   \n",
       "2      HOT TAKE Literally everyone has free time on t...   \n",
       "3      Fuck you Gordon Gordon I believed in you, I ca...   \n",
       "4      Canâ€™t find a picture Someone uploaded a ohoto ...   \n",
       "...                                                  ...   \n",
       "82237  Hurricane Delta (BECN) ðŸ”¥ \\nHurricane Delta is ...   \n",
       "82238  Made 40k on Nike. Next play? CROCS motherfucke...   \n",
       "82239  Please screenshot the whole timeline, not just...   \n",
       "82240  What is your price target for Tesla in 40 year...   \n",
       "82241  White House infected folks will infect Senator...   \n",
       "\n",
       "                                                  tokens       stocks  \n",
       "0      [okay, may, actually, retarded, defense, every...      [Other]  \n",
       "1      [beginning, virus, shit, everyone, talking, pu...      [Other]  \n",
       "2      [literally, everyone, free, time, hands, mean,...      [Other]  \n",
       "3      [gordon, believed, even, begin, describe, disa...      [Other]  \n",
       "4      [someone, uploaded, ohoto, stock, market, hist...      [Other]  \n",
       "...                                                  ...          ...  \n",
       "82237  [hurricane, delta, looking, like, going, hit, ...      [Other]  \n",
       "82238  [introduction, proof, lucky, way, going, struc...  [spy, tsla]  \n",
       "82239  [could, nut, retarded, failures, fine, really,...      [Other]  \n",
       "82240  [currently, max, roth, year, robinhood, specul...      [Other]  \n",
       "82241  [gonna, get, rona, means, checkies, stimulus, ...      [Other]  \n",
       "\n",
       "[82242 rows x 7 columns]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
